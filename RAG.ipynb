{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b523111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ceb116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Last_Refresh import get_last_refresh_date, update_last_refresh_date\n",
    "from BDD import save_embeddings_numpy, load_embeddings, append_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d49123",
   "metadata": {},
   "source": [
    "# API to fetch all VIE Id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d325621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_offers(limit=100, skip=0):\n",
    "    # URL de l'API\n",
    "    url = \"https://civiweb-api-prd.azurewebsites.net/api/Offers/search\"\n",
    "\n",
    "    # Corps de la requête (payload)\n",
    "    payload = {\n",
    "        \"limit\": limit,\n",
    "        \"skip\": skip,\n",
    "        \"latest\": [\"true\"],\n",
    "        \"method\": [\"null\"],\n",
    "        \"activitySectorId\": [],\n",
    "        \"missionsTypesIds\": [\"1\"],\n",
    "        \"missionsDurations\": [],\n",
    "        \"gerographicZones\": [],\n",
    "        \"countriesIds\": [],\n",
    "        \"studiesLevelId\": [],\n",
    "        \"companiesSizes\": [],\n",
    "        \"specializationsIds\": [],\n",
    "        \"entreprisesIds\": [0],\n",
    "        \"missionStartDate\": None,\n",
    "        \"query\": None\n",
    "    }\n",
    "\n",
    "    # Headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Effectuer la requête POST\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    # Vérifier le code de statut\n",
    "    response.raise_for_status()\n",
    "    # Afficher le résultat\n",
    "    print(f\"Code de statut: {response.status_code}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "#search_offers(10,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c4a7f",
   "metadata": {},
   "source": [
    "# API to fetch all the data for each ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd279d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offer_details(offer_id):\n",
    "    url = f\"https://civiweb-api-prd.azurewebsites.net/api/Offers/details/{offer_id}\"\n",
    "    # Headers (optionnel pour un GET simple)\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    # Effectuer la requête GET\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Vérifier le code de statut\n",
    "    response.raise_for_status()\n",
    "    print(\"   Récupération des données réussies\")\n",
    "    return response.json()\n",
    "\n",
    "#get_offer_details(227664)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9caeb",
   "metadata": {},
   "source": [
    "## Methods to compute application rate of the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3e48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_offer_data(offer_data):\n",
    "    \"\"\"Améliore les données avec des métriques calculées\"\"\"\n",
    "    candidates = offer_data.get('candidateCounter', 0)\n",
    "    views = offer_data.get('viewCounter', 0)\n",
    "    \n",
    "    # Calcul du taux de postulation\n",
    "    application_rate = 0\n",
    "    if views > 0:\n",
    "        application_rate = round((candidates / views) * 100, 1)\n",
    "    \n",
    "    # Catégorisation de la compétition\n",
    "    competition_level = \"FAIBLE\"\n",
    "    if application_rate > 10:\n",
    "        competition_level = \"ÉLEVÉE\"\n",
    "    elif application_rate > 5:\n",
    "        competition_level = \"MOYENNE\"\n",
    "    \n",
    "    return {\n",
    "        **offer_data,\n",
    "        \"application_rate\": application_rate,  \n",
    "        \"competition_level\": competition_level,  \n",
    "        \"candidates_count\": candidates,\n",
    "        \"views_count\": views\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea715e18",
   "metadata": {},
   "source": [
    "## Methods to clean the data, (Used in the creating chunk method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2414016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_offer_data(offer_data):\n",
    "    print(\"   Nettoyage des données\")\n",
    "    \"\"\"Nettoie et complète les données manquantes\"\"\"\n",
    "    return {\n",
    "        \"reference\": offer_data.get(\"reference\", \"N/A\"),\n",
    "        \"organizationName\": offer_data.get(\"organizationName\", \"Entreprise non spécifiée\"),\n",
    "        \"missionTitle\": offer_data.get(\"missionTitle\", \"Titre non spécifié\"),\n",
    "        \"missionDescription\": offer_data.get(\"missionDescription\", \"Description non disponible\"),\n",
    "        \"missionProfile\": offer_data.get(\"missionProfile\", \"Profil non spécifié\"),\n",
    "        \"countryNameEn\": offer_data.get(\"countryNameEn\", \"Pays non spécifié\"),\n",
    "        \"cityNameEn\": offer_data.get(\"cityNameEn\", \"Ville non spécifiée\"),\n",
    "        \"activitySectorN1\": offer_data.get(\"activitySectorN1\", \"Secteur non spécifié\"),\n",
    "        \"missionDuration\": offer_data.get(\"missionDuration\", \"Durée non spécifiée\"),\n",
    "        \"indemnite\": offer_data.get(\"indemnite\", \"Non spécifié\"),\n",
    "        \"missionStartDate\": offer_data.get(\"missionStartDate\", \"Date non spécifiée\"),\n",
    "        \"creationDate\": offer_data.get(\"creationDate\",\"Date non spécifiée\"),\n",
    "        \"contactEmail\": offer_data.get(\"contactEmail\", \"Email non disponible\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0fb015",
   "metadata": {},
   "source": [
    "# Creating chunks method\n",
    "\n",
    "We are dividing our chunks into content and metadata.\n",
    "\n",
    "We will only apply embedding on our content.\n",
    "\n",
    "Metadata will allow to use some filters when fetching the embeddings in the ChromaDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9ae181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_chunks_for_rag(offer_data):\n",
    "    \"\"\"Crée 2 chunks optimisés pour le RAG\"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    # 1. Nettoyer les données brutes\n",
    "    cleaned_offer = clean_offer_data(offer_data)\n",
    "\n",
    "    # 2. Calculer le taux de postulation à l'offre\n",
    "    enhanced_data = enhance_offer_data(offer_data)\n",
    "\n",
    "    # 2. Extraire les métadonnées depuis les données nettoyées\n",
    "    common_metadata = {\n",
    "        \"offer_reference\": cleaned_offer.get(\"reference\"),  \n",
    "        \"company\": cleaned_offer.get(\"organizationName\"),\n",
    "        \"title\": cleaned_offer.get(\"missionTitle\"),\n",
    "        \"country\": cleaned_offer.get(\"countryNameEn\"),\n",
    "        \"city\": cleaned_offer.get(\"cityNameEn\"),\n",
    "        \"sector\": cleaned_offer.get(\"activitySectorN1\"),\n",
    "        \"duration_months\": cleaned_offer.get(\"missionDuration\"),\n",
    "        \"salary_eur\": cleaned_offer.get(\"indemnite\"),\n",
    "        \"start_date\": cleaned_offer.get(\"missionStartDate\"),\n",
    "        \"creation_date\": cleaned_offer.get(\"creationDate\"),\n",
    "        \"contact_email\": cleaned_offer.get(\"contactEmail\"),\n",
    "        # Rest is comming from enhanced_data\n",
    "        \"application_rate\": enhanced_data[\"application_rate\"],\n",
    "        \"competition_level\": enhanced_data[\"competition_level\"], \n",
    "        \"candidates_count\": enhanced_data[\"candidates_count\"],\n",
    "        \"views_count\": enhanced_data[\"views_count\"]\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    # CHUNK 1: Description de la mission\n",
    "    chunk1_content = f\"\"\"Offre VIE {offer_data.get('reference')} - {offer_data.get('missionTitle')}\n",
    "    Entreprise: {offer_data.get('organizationName')}\n",
    "    Localisation: {offer_data.get('cityNameEn')}, {offer_data.get('countryNameEn')}\n",
    "    Secteur d'activité: {offer_data.get('activitySectorN1')}\n",
    "    Durée: {offer_data.get('missionDuration')} mois\n",
    "    Indemnité: {offer_data.get('indemnite')} € par mois\n",
    "    Description de la mission: {offer_data.get('missionDescription', 'Non spécifiée')}\"\"\"\n",
    "\n",
    "    new_chunk = {\n",
    "        \"content\": chunk1_content,\n",
    "        \"metadata\": {\n",
    "            **common_metadata,\n",
    "            \"chunk_type\": \"mission_description\",\n",
    "            \"chunk_id\": f\"{offer_data.get('id')}_description\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(f\"   Taille du chunk de Description (caractères) : {len(new_chunk['content'])}\")\n",
    "    chunks.append(new_chunk)\n",
    "    \n",
    "    # CHUNK 2: Profil recherché\n",
    "    chunk2_content = f\"\"\"Offre VIE {offer_data.get('reference')} - {offer_data.get('missionTitle')}\n",
    "    Entreprise: {offer_data.get('organizationName')}\n",
    "    Localisation: {offer_data.get('cityNameEn')}, {offer_data.get('countryNameEn')}\n",
    "    Secteur d'activité: {offer_data.get('activitySectorN1')}\n",
    "    Profil recherché: {offer_data.get('missionProfile', 'Non spécifié')}\"\"\"\n",
    "\n",
    "    new_chunk2 = {\n",
    "        \"content\": chunk2_content,\n",
    "        \"metadata\": {\n",
    "            **common_metadata,\n",
    "            \"chunk_type\": \"candidate_profile\",\n",
    "            \"chunk_id\": f\"{offer_data.get('id')}_profile\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    chunks.append(new_chunk2)\n",
    "    print(f\"   Taille du chunk de Profil (caractères) : {len(new_chunk2['content'])}\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b65f7",
   "metadata": {},
   "source": [
    "# Create the embeding of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af59a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration OpenAI\n",
    "api_key = os.environ.get(\"API_KEY_OPENAI\")\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674a7e3",
   "metadata": {},
   "source": [
    "###  Method to embed the question that we will ask the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573b4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Crée un embedding pour un texte avec OpenAI\n",
    "    \n",
    "    Args:\n",
    "        text: Le texte à embedder\n",
    "        \n",
    "    Returns:\n",
    "        Liste de floats représentant l'embedding\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"  # 1536 dimensions, $0.02/1M tokens\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b0cb4",
   "metadata": {},
   "source": [
    "###  Method to create our embeddings dataset\n",
    "\n",
    "We are using batch to optimize API calls and reduce latency. \n",
    "\n",
    "This approach processes multiple texts in a single request instead of making individual calls for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b9ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_batch(chunks: List[Dict], batch_size: int = 100) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Crée les embeddings pour tous les chunks avec traitement par batch\n",
    "    \n",
    "    Args:\n",
    "        chunks: Liste des chunks à embedder\n",
    "        batch_size: Nombre de chunks à traiter par batch (max 100 pour OpenAI)\n",
    "        \n",
    "    Returns:\n",
    "        Liste des chunks enrichis avec leurs embeddings\n",
    "    \"\"\"\n",
    "    enriched_chunks = []\n",
    "    \n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        print(f\"Traitement du batch {i//batch_size + 1} ({len(batch)} chunks)...\")\n",
    "        \n",
    "        # Extraire les contenus textuels\n",
    "        texts = [chunk[\"content\"] for chunk in batch]\n",
    "        \n",
    "        # Créer les embeddings en batch (plus efficace)\n",
    "        response = client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        \n",
    "        # Associer chaque embedding à son chunk\n",
    "        for j, chunk in enumerate(batch):\n",
    "            enriched_chunk = chunk.copy()\n",
    "            enriched_chunk[\"embedding\"] = response.data[j].embedding\n",
    "            enriched_chunks.append(enriched_chunk)\n",
    "        \n",
    "        # Rate limiting (optionnel mais recommandé)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    print(f\"✅ {len(enriched_chunks)} embeddings créés\")\n",
    "    return enriched_chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f371a9",
   "metadata": {},
   "source": [
    "# Creation of the Pipeline : create the dataset embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12cf07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PIPELINE RAG - OFFRES VIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ÉTAPE 1: Rechercher les offres\n",
    "print(\"\\n[1/5] Recherche des offres VIE...\")\n",
    "offers_response = search_offers(limit=10000)  # Ajustez la limite selon vos besoins\n",
    "\n",
    "# La structure de retour peut varier, adaptez selon l'API\n",
    "# Supposons que l'API retourne une liste d'IDs ou d'objets simplifiés\n",
    "offer_ids = []\n",
    "offer_ids = [item.get(\"id\") for item in offers_response[\"result\"] if item.get(\"id\") is not None]\n",
    "\n",
    "print(f\"✅ {len(offer_ids)} offres trouvées\")\n",
    "\n",
    "# ÉTAPE 2: Récupérer les détails et créer les chunks\n",
    "print(\"\\n[2/5] Récupération des détails et création des chunks...\")\n",
    "all_chunks = []\n",
    "    \n",
    "\n",
    "for i, offer_id in enumerate(offer_ids, 1):  \n",
    "    try:\n",
    "        print(f\"  Traitement offre {i}/{len(offer_ids)}: {offer_id}\")\n",
    "        offer_details = get_offer_details(offer_id)\n",
    "        chunks = create_chunks_for_rag(offer_details)\n",
    "        all_chunks.extend(chunks)\n",
    "        time.sleep(0.2)  # Rate limiting pour l'API Civiweb\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  Erreur pour l'offre {offer_id}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"✅ {len(all_chunks)} chunks créés\\n\")\n",
    "\n",
    "# ÉTAPE 3: Créer les embeddings\n",
    "print(\"\\n[3/5] Création des embeddings OpenAI...\")\n",
    "chunks_with_embeddings = create_embeddings_batch(all_chunks)\n",
    "\n",
    "\n",
    "print(\"\\n[4/5] Sauvegarde des nouvelles données...\")\n",
    "save_embeddings_numpy(chunks_with_embeddings, \"vie_embeddings.npz\")\n",
    "\n",
    "# We save the current date\n",
    "update_last_refresh_date()\n",
    "\n",
    "# ÉTAPE 5: Statistiques\n",
    "print(\"\\n[5/5] Statistiques finales\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Nombre de nouvelles offres traitées: {len(offer_ids)}\")\n",
    "print(f\"Nombre de nouveaux chunks: {len(chunks_with_embeddings)}\")\n",
    "print(f\"Dimension des nouveaux embeddings: {len(chunks_with_embeddings[0]['embedding'])}\")\n",
    "\n",
    "# Calculer la taille totale\n",
    "total_tokens = sum(len(c['content']) // 4 for c in chunks_with_embeddings)\n",
    "print(f\"Tokens estimés: ~{total_tokens:,}\")\n",
    "print(f\"Coût estimé: ~${(total_tokens / 1_000_000) * 0.02:.4f}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de1645",
   "metadata": {},
   "source": [
    "# Creation of the Pipeline : Add new embeddings in the curent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41761b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PIPELINE RAG - Nouvelles OFFRES VIE\n",
      "================================================================================\n",
      "\n",
      "[1/5] Recherche des offres VIE...\n",
      "Code de statut: 200\n",
      "📅 Dernier rafraîchissement: 2025-10-15 13:56:29.187000\n",
      "✅ 32 nouvelles offres depuis le dernier rafraîchissement\n",
      "✅ Last_refresh mis à jour: 2025-10-16T12:11:01.703\n",
      "\n",
      "[2/5] Récupération des détails et création des chunks...\n",
      "  Traitement offre 1/32: 230924\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1921\n",
      "   Taille du chunk de Profil (caractères) : 1701\n",
      "✅ 2 chunks créés\n",
      "\n",
      "  Traitement offre 2/32: 230960\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1817\n",
      "   Taille du chunk de Profil (caractères) : 861\n",
      "✅ 4 chunks créés\n",
      "\n",
      "  Traitement offre 3/32: 230928\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 3896\n",
      "   Taille du chunk de Profil (caractères) : 1259\n",
      "✅ 6 chunks créés\n",
      "\n",
      "  Traitement offre 4/32: 230949\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2919\n",
      "   Taille du chunk de Profil (caractères) : 928\n",
      "✅ 8 chunks créés\n",
      "\n",
      "  Traitement offre 5/32: 230950\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2992\n",
      "   Taille du chunk de Profil (caractères) : 639\n",
      "✅ 10 chunks créés\n",
      "\n",
      "  Traitement offre 6/32: 230955\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2092\n",
      "   Taille du chunk de Profil (caractères) : 625\n",
      "✅ 12 chunks créés\n",
      "\n",
      "  Traitement offre 7/32: 230962\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2074\n",
      "   Taille du chunk de Profil (caractères) : 1011\n",
      "✅ 14 chunks créés\n",
      "\n",
      "  Traitement offre 8/32: 230938\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1793\n",
      "   Taille du chunk de Profil (caractères) : 905\n",
      "✅ 16 chunks créés\n",
      "\n",
      "  Traitement offre 9/32: 230936\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1793\n",
      "   Taille du chunk de Profil (caractères) : 905\n",
      "✅ 18 chunks créés\n",
      "\n",
      "  Traitement offre 10/32: 230952\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2989\n",
      "   Taille du chunk de Profil (caractères) : 1031\n",
      "✅ 20 chunks créés\n",
      "\n",
      "  Traitement offre 11/32: 230954\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1498\n",
      "   Taille du chunk de Profil (caractères) : 516\n",
      "✅ 22 chunks créés\n",
      "\n",
      "  Traitement offre 12/32: 230953\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1493\n",
      "   Taille du chunk de Profil (caractères) : 518\n",
      "✅ 24 chunks créés\n",
      "\n",
      "  Traitement offre 13/32: 230961\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2999\n",
      "   Taille du chunk de Profil (caractères) : 195\n",
      "✅ 26 chunks créés\n",
      "\n",
      "  Traitement offre 14/32: 230947\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2500\n",
      "   Taille du chunk de Profil (caractères) : 1067\n",
      "✅ 28 chunks créés\n",
      "\n",
      "  Traitement offre 15/32: 230946\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2342\n",
      "   Taille du chunk de Profil (caractères) : 684\n",
      "✅ 30 chunks créés\n",
      "\n",
      "  Traitement offre 16/32: 230940\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2386\n",
      "   Taille du chunk de Profil (caractères) : 841\n",
      "✅ 32 chunks créés\n",
      "\n",
      "  Traitement offre 17/32: 230934\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2470\n",
      "   Taille du chunk de Profil (caractères) : 2468\n",
      "✅ 34 chunks créés\n",
      "\n",
      "  Traitement offre 18/32: 230923\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2414\n",
      "   Taille du chunk de Profil (caractères) : 715\n",
      "✅ 36 chunks créés\n",
      "\n",
      "  Traitement offre 19/32: 230941\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 4091\n",
      "   Taille du chunk de Profil (caractères) : 1257\n",
      "✅ 38 chunks créés\n",
      "\n",
      "  Traitement offre 20/32: 230932\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 615\n",
      "   Taille du chunk de Profil (caractères) : 1315\n",
      "✅ 40 chunks créés\n",
      "\n",
      "  Traitement offre 21/32: 230945\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 987\n",
      "   Taille du chunk de Profil (caractères) : 1065\n",
      "✅ 42 chunks créés\n",
      "\n",
      "  Traitement offre 22/32: 230939\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1793\n",
      "   Taille du chunk de Profil (caractères) : 905\n",
      "✅ 44 chunks créés\n",
      "\n",
      "  Traitement offre 23/32: 230937\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1793\n",
      "   Taille du chunk de Profil (caractères) : 905\n",
      "✅ 46 chunks créés\n",
      "\n",
      "  Traitement offre 24/32: 230935\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1793\n",
      "   Taille du chunk de Profil (caractères) : 905\n",
      "✅ 48 chunks créés\n",
      "\n",
      "  Traitement offre 25/32: 230933\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1793\n",
      "   Taille du chunk de Profil (caractères) : 905\n",
      "✅ 50 chunks créés\n",
      "\n",
      "  Traitement offre 26/32: 230943\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2992\n",
      "   Taille du chunk de Profil (caractères) : 847\n",
      "✅ 52 chunks créés\n",
      "\n",
      "  Traitement offre 27/32: 230927\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2657\n",
      "   Taille du chunk de Profil (caractères) : 616\n",
      "✅ 54 chunks créés\n",
      "\n",
      "  Traitement offre 28/32: 230948\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 1734\n",
      "   Taille du chunk de Profil (caractères) : 534\n",
      "✅ 56 chunks créés\n",
      "\n",
      "  Traitement offre 29/32: 230944\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2355\n",
      "   Taille du chunk de Profil (caractères) : 2159\n",
      "✅ 58 chunks créés\n",
      "\n",
      "  Traitement offre 30/32: 230942\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2374\n",
      "   Taille du chunk de Profil (caractères) : 2505\n",
      "✅ 60 chunks créés\n",
      "\n",
      "  Traitement offre 31/32: 230929\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 809\n",
      "   Taille du chunk de Profil (caractères) : 1504\n",
      "✅ 62 chunks créés\n",
      "\n",
      "  Traitement offre 32/32: 230925\n",
      "   Récupération des données réussies\n",
      "   Nettoyage des données\n",
      "   Taille du chunk de Description (caractères) : 2614\n",
      "   Taille du chunk de Profil (caractères) : 3448\n",
      "✅ 64 chunks créés\n",
      "\n",
      "\n",
      "[3/5] Création des embeddings OpenAI...\n",
      "Traitement du batch 1 (64 chunks)...\n",
      "✅ 64 embeddings créés\n",
      "\n",
      "[4/5] Sauvegarde des données...\n",
      "✅ Embeddings sauvegardés dans vie_embeddings.npz\n",
      "   Shape: (64, 1536)\n",
      "\n",
      "[5/5] Statistiques finales\n",
      "================================================================================\n",
      "Nombre d'offres traitées: 32\n",
      "Nombre de chunks: 64\n",
      "Dimension des embeddings: 1536\n",
      "Tokens estimés: ~26,608\n",
      "Coût estimé: ~$0.0005\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PIPELINE RAG - Nouvelles OFFRES VIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ÉTAPE 1: Rechercher les offres\n",
    "print(\"\\n[1/5] Recherche des offres VIE...\")\n",
    "offers_response = search_offers(limit=100000)  # Ajustez la limite selon vos besoins\n",
    "\n",
    "last_refresh_date = get_last_refresh_date()\n",
    "print(f\"📅 Dernier rafraîchissement: {last_refresh_date}\")\n",
    "\n",
    "offer_ids = []\n",
    "offer_ids = [\n",
    "    item.get(\"id\") for item in offers_response[\"result\"] \n",
    "    if (item.get(\"id\") is not None and \n",
    "        datetime.fromisoformat(item.get(\"creationDate\")) > last_refresh_date)\n",
    "]\n",
    "\n",
    "print(f\"✅ {len(offer_ids)} nouvelles offres depuis le dernier rafraîchissement\")\n",
    "# Mettre à jour Last_refresh après le traitement\n",
    "update_last_refresh_date()\n",
    "\n",
    "# ÉTAPE 2: Récupérer les détails et créer les chunks\n",
    "print(\"\\n[2/5] Récupération des détails et création des chunks...\")\n",
    "all_chunks = []\n",
    "    \n",
    "\n",
    "for i, offer_id in enumerate(offer_ids, 1):  \n",
    "    try:\n",
    "        print(f\"  Traitement offre {i}/{len(offer_ids)}: {offer_id}\")\n",
    "        offer_details = get_offer_details(offer_id)\n",
    "        chunks = create_chunks_for_rag(offer_details)\n",
    "        all_chunks.extend(chunks)\n",
    "        time.sleep(0.2)  # Rate limiting pour l'API Civiweb\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  Erreur pour l'offre {offer_id}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"✅ {len(all_chunks)} chunks créés\\n\")\n",
    "\n",
    "# ÉTAPE 3: Créer les embeddings\n",
    "print(\"\\n[3/5] Création des embeddings OpenAI...\")\n",
    "chunks_with_embeddings = create_embeddings_batch(all_chunks)\n",
    "\n",
    "\n",
    "print(\"\\n[4/5] Sauvegarde des données...\")\n",
    "save_embeddings_numpy(chunks_with_embeddings, \"vie_embeddings.npz\")\n",
    "\n",
    "\n",
    "# ÉTAPE 5: Statistiques\n",
    "print(\"\\n[5/5] Statistiques finales\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Nombre d'offres traitées: {len(offer_ids)}\")\n",
    "print(f\"Nombre de chunks: {len(chunks_with_embeddings)}\")\n",
    "print(f\"Dimension des embeddings: {len(chunks_with_embeddings[0]['embedding'])}\")\n",
    "\n",
    "# Calculer la taille totale\n",
    "total_tokens = sum(len(c['content']) // 4 for c in chunks_with_embeddings)\n",
    "print(f\"Tokens estimés: ~{total_tokens:,}\")\n",
    "print(f\"Coût estimé: ~${(total_tokens / 1_000_000) * 0.02:.4f}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3f7eb",
   "metadata": {},
   "source": [
    "# Now let's ask a question and embed it !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fcfa511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"Je Cherche un VIE de minimum 1 an, je suis orientée en Data et IA mais j'aime aussi faire du développement web. Peux tu me donner les meilleurs offres pour moi. Sachant que je cherche une offre pour Octobre 2025\"\n",
    "question = \"Je Cherche un VIE de minimum 1 an, je suis orientée en Data et IA mais j'aime aussi faire du développement web. Peux tu me donner les meilleurs offres pour moi. Si possible priorise les offres en corée du sud\"\n",
    "# question = \"Je Cherche un VIE en Asie de l'Est  pour faire de la data ou de l'IA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd87e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d5200",
   "metadata": {},
   "source": [
    "# Comparison bewtween our embedding database and our embedding question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "160b890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def find_closest_embedding(\n",
    "    question_embeddings: np.array,\n",
    "    text_embeddings: np.ndarray,\n",
    "    top_n: int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    # Calculer les similarités avec tous les embeddings\n",
    "    similarites = cosine_similarity(question_embeddings, text_embeddings)[0]\n",
    "\n",
    "    # Récupérer les top_n index et scores\n",
    "    top_index = np.argsort(similarites)[-top_n:][::-1]  # Tri décroissant\n",
    "    top_scores = similarites[top_index]\n",
    "\n",
    "    return top_scores, top_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "142df887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 embeddings les plus proches:\n",
      " \n",
      "- Embedding 1: Score = 0.6616\n",
      "- Embedding 7: Score = 0.6284\n",
      "- Embedding 0: Score = 0.6262\n",
      "- Embedding 45: Score = 0.6128\n",
      "- Embedding 47: Score = 0.6117\n",
      "- Embedding 17: Score = 0.6117\n",
      "- Embedding 15: Score = 0.6110\n",
      "- Embedding 49: Score = 0.6109\n",
      "- Embedding 43: Score = 0.6108\n",
      "- Embedding 39: Score = 0.6096\n"
     ]
    }
   ],
   "source": [
    "loaded_embeddings = load_embeddings(\"vie_embeddings.npz\")\n",
    "text_embeddings = loaded_embeddings['embeddings']\n",
    "top_scores, top_index = find_closest_embedding(question_embeddings, text_embeddings, 10)\n",
    "\n",
    "top_results = []\n",
    "\n",
    "print(\"Top 10 embeddings les plus proches:\\n \")\n",
    "for score, idx in zip(top_scores, top_index):\n",
    "    top_results.append(f\"<OFFRE {idx}>\")\n",
    "    print(f\"- Embedding {idx}: Score = {score:.4f}\")\n",
    "    #print(f\"metadata : {loaded_embeddings['metadata'][idx]}\")\n",
    "    #print(f\"contents : {loaded_embeddings['contents'][idx]}\")\n",
    "    metadata = json.dumps(loaded_embeddings['metadata'][idx])\n",
    "    value = metadata+loaded_embeddings['contents'][idx]\n",
    "    top_results.append(value)\n",
    "    top_results.append(f\"</OFFRE {idx}>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43189f",
   "metadata": {},
   "source": [
    "# We Use a LLM to answer our question using the 10 closest embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cf51178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parmi les offres disponibles dans le contexte, l'offre qui se rapproche le plus de votre profil orienté en Data et IA est l'**OFFRE 1** :\n",
      "\n",
      "- **Titre** : VIE SOLUTION ARCHITECT IA JUNIOR (H/F)\n",
      "- **Entreprise** : THUASNE\n",
      "- **Localisation** : KANSAS CITY, UNITED STATES\n",
      "- **Durée** : 18 mois\n",
      "- **Indemnité** : 3393 € par mois\n",
      "- **Profil recherché** : \n",
      "  - Diplôme Bac+5 en informatique, science des données, IA ou domaine connexe.\n",
      "  - 1 à 3 ans d'expérience dans des projets d'IA.\n",
      "  - Compétences en Python, concepts d'IA/ML, familiarité avec TensorFlow, PyTorch, Scikit-learn, et plateformes de cloud computing (AWS, Azure, GCP).\n",
      "\n",
      "Malheureusement, aucune offre spécifiquement en Corée du Sud n'est mentionnée dans le contexte fourni.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Concaténer les top-N segments en un seul contexte\n",
    "context = \"\\n\".join(top_results)\n",
    "\n",
    "prompt = f\"Contexte :\\n{context}\\n\\nQuestion : {question}\\nRéponse :\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Tu es un assistant spécialisé dans la recherche d'information à partir de documents fournis. Tes réponses doivent absolument provenir du contexte fourni.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
